# -*- coding: utf-8 -*-
"""BESTMODELS_strat1_h2o

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mrxGSNyk629VckgR4eJWdodXzBUTqfUD
"""

! java -version
! pip install h2o --quiet
import numpy as np
import pandas as pd
from abc import ABC, abstractmethod
from typing import Generator, Tuple
import scipy.stats as ss
from scipy.stats import norm
import h2o
from sklearn.metrics import mutual_info_score
from h2o.automl import H2OAutoML
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from joblib import Parallel, delayed
import math
import itertools as itt
from typing import Generator, Tuple, List, Dict
import matplotlib.pyplot as plt
import seaborn as sns
import os
import glob

def calc_pnl(close: pd.Series, positions: int = 1) -> pd.Series:
    return ((close).diff() * positions.shift()).fillna(0)

def calc_sharpe(pnl):
    pnl.fillna(0)
    annualised_pnl = pnl.resample("D").sum()
    return (annualised_pnl.mean() / annualised_pnl.std()) * np.sqrt(252)

class CrossVal(ABC):
    @abstractmethod
    def create_splits(
        self,
        data: pd.DataFrame,
        times: pd.DataFrame,
        n_groups: int,
        k_test_groups: int = 1,
    ) -> List[Dict]:
        pass


class CombPurgeEmbargoKFold(CrossVal):
    def create_splits(
        self,
        data: pd.DataFrame,
        times: pd.DataFrame,
        n_groups: int = 5,
        k_test_groups: int = 2,
    ) -> List[Dict]:
        indices = np.arange(len(data))
        split_segments = np.array_split(indices, n_groups)
        combinations_ = list(itt.combinations(range(n_groups), k_test_groups))
        splits = []
        for test_groups in combinations_:
            test_indices = {f"group_{i + 1}": split_segments[i] for i in range(n_groups)}
            train_indices = np.setdiff1d(
                indices,
                np.concatenate([test_indices[f"group_{g + 1}"] for g in test_groups]),
            )
            times_train = times.iloc[train_indices]
            times_test = times.iloc[
                np.concatenate([test_indices[f"group_{g + 1}"] for g in test_groups])
            ]
            splits.append(
                {
                    "train_indices": train_indices,
                    "test_indices": {
                        f"group_{g + 1}": test_indices[f"group_{g + 1}"]
                        for g in test_groups
                    },
                    "times_train": times_train[["start_time", "end_time"]],
                    "times_test": times_test[["start_time", "end_time"]],
                }
            )
        return splits




class KFold(CrossVal):
    def create_splits(
        self,
        data: pd.DataFrame,
        times: pd.DataFrame,
        n_groups: int = 5,
        k_test_groups: int = 2,
    ) -> List[Dict]:
        indices = np.arange(len(data))
        split_segments = np.array_split(indices, n_groups)
        combinations_ = list(itt.combinations(range(n_groups), 1))
        splits = []

        for test_groups in combinations_:
            test_indices = {
                f"group_{i + 1}": split_segments[i] for i in range(n_groups)
            }
            train_indices = np.setdiff1d(
                indices,
                np.concatenate(
                    [test_indices[f"group_{g + 1}"] for g in test_groups]
                ),
            )

            times_train = times.iloc[train_indices]
            times_test = times.iloc[
                np.concatenate(
                    [test_indices[f"group_{g + 1}"] for g in test_groups]
                )
            ]

            splits.append(
                {
                    "train_indices": train_indices,
                    "test_indices": {
                        f"group_{g + 1}": test_indices[f"group_{g + 1}"]
                        for g in test_groups
                    },
                    "times_train": times_train[["start_time", "end_time"]],
                    "times_test": times_test[["start_time", "end_time"]],
                }
            )

        return splits

class PurgedKFold(CrossVal):
    def create_splits(
        self,
        data: pd.DataFrame,
        times: pd.DataFrame,
        n_groups: int = 5,
    ) -> List[Dict]:
        indices = np.arange(len(data))
        split_segments = np.array_split(indices, n_groups)
        combinations_ = list(itt.combinations(range(n_groups), 1))
        splits = []
        for test_groups in combinations_:
            test_indices = {f"group_{i + 1}": split_segments[i] for i in range(n_groups)}
            train_indices = np.setdiff1d(
                indices,
                np.concatenate([test_indices[f"group_{g + 1}"] for g in test_groups]),
            )
            times_train = times.iloc[train_indices]
            times_test = times.iloc[
                np.concatenate([test_indices[f"group_{g + 1}"] for g in test_groups])
            ]
            splits.append(
                {
                    "train_indices": train_indices,
                    "test_indices": {
                        f"group_{g + 1}": test_indices[f"group_{g + 1}"]
                        for g in test_groups
                    },
                    "times_train": times_train[["start_time", "end_time"]],
                    "times_test": times_test[["start_time", "end_time"]],
                }
            )
        return splits


class RollingWindow(CrossVal):
    def create_splits(
        self,
        data: pd.DataFrame,
        times: pd.DataFrame,
        train_size: int,
        step_size: int,
    ) -> List[Dict]:
        indices = np.arange(len(data))
        splits = []
        start = 0
        group_counter = 1  # Initialise group counter

        while start + train_size + step_size <= len(data):
            train_indices = indices[start: start + train_size]
            test_indices = indices[start + train_size: start + train_size + step_size]
            times_train = times.iloc[train_indices]
            times_test = times.iloc[test_indices]

            splits.append(
                {
                    "train_indices": train_indices,
                    "test_indices": {f"group_{group_counter}": test_indices},
                    "times_train": times_train[["start_time", "end_time"]],
                    "times_test": times_test[["start_time", "end_time"]],
                }
            )

            start += step_size
            group_counter += 1  # Increment the group number

        return splits



class ExpandingWindow(CrossVal):
    def create_splits(
        self,
        data: pd.DataFrame,
        times: pd.DataFrame,
        initial_train_size: int,
        step_size: int,
    ) -> List[Dict]:
        indices = np.arange(len(data))
        splits = []
        train_end = initial_train_size
        group_counter = 1  # Initialise group counter

        while train_end + step_size <= len(data):
            train_indices = indices[:train_end]
            test_indices = indices[train_end: train_end + step_size]
            times_train = times.iloc[train_indices]
            times_test = times.iloc[test_indices]

            splits.append(
                {
                    "train_indices": train_indices,
                    "test_indices": {f"group_{group_counter}": test_indices},
                    "times_train": times_train[["start_time", "end_time"]],
                    "times_test": times_test[["start_time", "end_time"]],
                }
            )

            train_end += step_size
            group_counter += 1  # Increment the group number

        return splits

def form_backtest_paths(predictions):
    paths = {}

    for model_id, model_predictions in predictions.items():
        # Initialize model paths
        model_paths = {}
        instance_counters = {}

        # First pass: collect all unique group names
        group_names = set()
        for split_preds in model_predictions.values():
            group_names.update(split_preds.keys())

        # Initialize counters for all groups
        instance_counters = {group: 0 for group in group_names}

        # Second pass: organize predictions into paths
        for split, split_preds in model_predictions.items():
            for group_name, preds in split_preds.items():
                current_instance = instance_counters[group_name]
                path_key = f"path_{current_instance + 1}"

                # Initialize path if it doesn't exist
                if path_key not in model_paths:
                    model_paths[path_key] = {}

                # Store predictions and dates in the original format
                model_paths[path_key][group_name] = {
                    "predictions": preds["predictions"],
                    "dates": preds["date"]  # Note: storing as 'dates' to match old format
                }

                instance_counters[group_name] += 1

        paths[model_id] = model_paths

    return paths

def apply_embargo(splits, total_data_size, pct_embargo=0.1):
    """Apply purging and embargo to splits."""
    embargo_size = int(total_data_size * pct_embargo)
    print(" embargo size:", embargo_size)

    def get_continuous_ranges(arr):
        """Get continuous ranges from sorted indices."""
        ranges = []
        start = arr[0]
        for i in range(1, len(arr)):
            if arr[i] != arr[i - 1] + 1:
                ranges.append((start, arr[i - 1]))
                start = arr[i]
        ranges.append((start, arr[-1]))
        return ranges

    def apply_embargo_to_range(train_range, last_test_index):
        """Apply embargo by removing the first part of the train range if needed."""
        if train_range[0] == last_test_index + 1:
            new_start = min(train_range[0] + embargo_size, train_range[1] + 1)
            return (new_start, train_range[1])
        return train_range

    embargoed_splits = []

    for split in splits:
        new_train_ranges = []
        train_ranges = get_continuous_ranges(np.array(split["train_indices"]))
        last_test_indices = [max(indices) for indices in split["test_indices"].values()]

        for train_range in train_ranges:
            embargoed_range = train_range
            for last_test_index in last_test_indices:
                embargoed_range = apply_embargo_to_range(embargoed_range, last_test_index)
            new_train_ranges.append(embargoed_range)

        purged_train_indices = []
        for start, end in new_train_ranges:
            purged_train_indices.append(np.arange(start, end + 1))

        new_train_indices = np.concatenate(purged_train_indices)
        # Create mask for selecting times based on original indices
        indices_mask = np.isin(split["train_indices"], new_train_indices)
        new_times_train = split["times_train"].iloc[indices_mask].copy()

        embargoed_splits.append({
            "train_indices": new_train_indices,
            "test_indices": split["test_indices"],
            "times_train": new_times_train,
            "times_test": split["times_test"],
        })

    return embargoed_splits



def apply_purging(splits):
    purged_splits = []

    for split in splits:
        train_indices = split["train_indices"]
        train_times = split["times_train"]
        test_times = split["times_test"]

        train_start = train_times['start_time'].values
        train_end = train_times['end_time'].values
        keep_train_mask = np.ones(len(train_indices), dtype=bool)

        for test_idx in range(len(test_times)):
            test_start = test_times['start_time'].iloc[test_idx]
            test_end = test_times['end_time'].iloc[test_idx]

            # Calculate overlap for current length of train indices
            overlap = (train_start < test_end) & (train_end > test_start)
            keep_train_mask &= ~overlap

        purged_train_indices = train_indices[keep_train_mask]
        purged_times_train = train_times.iloc[keep_train_mask]

        purged_splits.append({
            "train_indices": purged_train_indices,
            "test_indices": split["test_indices"],
            "times_train": purged_times_train,
            "times_test": test_times,
        })

    return purged_splits

import seaborn as sns

def final_plot_splits(splits):
    plt.figure(figsize=(16, 2))
    sns.set_style('darkgrid')

    # For each split
    for i, split in enumerate(splits):
        # Plot train indices
        plt.scatter(
            split['train_indices'],
            [i] * len(split['train_indices']),
            color='lightgrey',
            s=5,
            label='Train' if i == 0 else ""
        )

        # Plot test indices
        test_indices = np.concatenate([indices for indices in split['test_indices'].values()])
        plt.scatter(
            test_indices,
            [i] * len(test_indices),
            color='red',
            s=5,
            label='Test' if i == 0 else ""
        )

    plt.yticks(range(len(splits)), [f'Split {i+1}' for i in range(len(splits))])
    plt.xlabel('Index')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()

def identify_best_models_h2o(
    cv_class: CrossVal,
    data: pd.DataFrame,
    times: pd.DataFrame,
    pct_embargo: float = 0.01,
    n_groups: int = 5,
    k_test_groups: int = 2,
    train_size: int = None,
    step_size: int = None,
    initial_train_size: int = None
) -> pd.DataFrame:
    if isinstance(cv_class, CombPurgeEmbargoKFold):
        splits = cv_class.create_splits(data, times, n_groups=n_groups, k_test_groups=k_test_groups)
        embargoed_splits = apply_embargo(splits, total_data_size = len(data), pct_embargo=pct_embargo)
        purged_splits = apply_purging(embargoed_splits)
    elif isinstance(cv_class, PurgedKFold):
        splits = cv_class.create_splits(data, times, n_groups=n_groups)
        embargoed_splits = apply_embargo(splits, total_data_size = len(data), pct_embargo=pct_embargo)
        purged_splits = apply_purging(embargoed_splits)
    elif isinstance(cv_class, KFold):
        splits = cv_class.create_splits(data, times, n_groups=n_groups, k_test_groups=k_test_groups)
        purged_splits = splits
    elif isinstance(cv_class, RollingWindow):
        splits = cv_class.create_splits(data, times, train_size=train_size, step_size=step_size)
        purged_splits = apply_purging(splits)
    elif isinstance(cv_class, ExpandingWindow):
        splits = cv_class.create_splits(data, times, initial_train_size=initial_train_size, step_size=step_size)
        purged_splits = apply_purging(splits)

    # final_plot_splits(purged_splits)

    predictions = {}

    for model in loaded_models[currency]:
        model_id = model.model_id
        print(f'\n  model: {model_id}')
        model_predictions = {}

        for i, split in enumerate(purged_splits):
            print(f"   split: {i + 1} / {len(purged_splits)}")
            train_indices = split["train_indices"]
            test_indices_dict = split["test_indices"]

            train_data = data.iloc[train_indices]
            train_h2o = h2o.H2OFrame(train_data)
            y = "target"
            x = train_h2o.columns
            x.remove(y)
            train_h2o[y] = train_h2o[y].asfactor()
            model.train(x=x, y=y, training_frame=train_h2o)
            print('   > finished training')
            split_preds = {}
            for group_name, test_indices in test_indices_dict.items():
                test_data = data.iloc[test_indices]
                test_h2o = h2o.H2OFrame(test_data)
                test_h2o[y] = test_h2o[y].asfactor()
                predictions_h2o = model.predict(test_h2o)
                pred_df = predictions_h2o.as_data_frame(use_multi_thread=True)
                start_times = times.iloc[test_indices]["start_time"].values

                split_preds[group_name] = {"predictions": pred_df, "date": start_times}

            model_predictions[f"split_{i + 1}"] = split_preds
            print('   > finished predicting')

        predictions[model_id] = model_predictions

    all_paths = form_backtest_paths(predictions)

    concatenated_paths = {}
    for model_key, paths in all_paths.items():
        concatenated_paths[model_key] = {}
        for path_key, groups in paths.items():
            concatenated_predictions = np.concatenate([groups[group_name]["predictions"].values for group_name in groups])
            concatenated_date = np.concatenate([groups[group_name]["date"] for group_name in groups])
            concatenated_paths[model_key][path_key] = {
                "predictions": concatenated_predictions,
                "date": concatenated_date,
            }

    sharpe_ratios = {}
    for model_key, paths in concatenated_paths.items():
        sharpe_ratios[model_key] = {}
        for path_key, data in paths.items():
            predictions = data["predictions"][:, 0]
            date = data["date"]
            positions = np.where(predictions == 1, 1, np.where(predictions == -1, -1, 0))
            positions_series = pd.Series(positions, index=pd.to_datetime(date))
            filtered_close = close[close.index.isin(positions_series.index)].squeeze()
            strat_returns = calc_pnl(filtered_close, positions_series[filtered_close.index])
            sharpe_ratio = calc_sharpe(strat_returns.dropna())
            sharpe_ratios[model_key][path_key] = sharpe_ratio

    all_models_cpcv = []
    for model_key, paths in sharpe_ratios.items():
        row = {"model": model_key}
        for path_key, sharpe_value in paths.items():
            row[path_key] = sharpe_value
        average_sharpe = sum(paths.values()) / len(paths) if paths else 0
        row["average_sharpe"] = average_sharpe
        all_models_cpcv.append(row)

    sharpe_df = pd.DataFrame(all_models_cpcv)
    return sharpe_df

currency_input_directory = '/content/drive/MyDrive/1. learning/1datasci/financial ML/project 1. backtest overfitting forex/results_forex/LABELS_strat1'

feature_files = glob.glob(os.path.join(currency_input_directory, "*_features.csv"))

currency_data = {}

for file in feature_files:
    currency = os.path.basename(file).split('_')[0]
    print(f"\n===================================")
    print(f" processing currency: {currency}")

    features = pd.read_csv(f"{currency_input_directory}/{currency}_features.csv", index_col=0)
    target = pd.read_csv(f"{currency_input_directory}/{currency}_target.csv", index_col=0)
    close = pd.read_csv(f"{currency_input_directory}/{currency}_close.csv", index_col=0)
    times = pd.read_csv(f"{currency_input_directory}/{currency}_times.csv", index_col=0)
    features.index = pd.to_datetime(features.index, format='mixed')
    times.index = pd.to_datetime(times.index)
    close.index = pd.to_datetime(close.index)
    target.index = pd.to_datetime(target.index)
    print(len(features), len(target), len(close), len(times))

    features_train, features_test, target_train, target_test = train_test_split(
        features, target, train_size=0.7, shuffle=False)
    train = pd.concat([features_train, target_train], axis=1)

    currency_data[currency] = {
        'train': train,
        'features': features,
        'target': target,
        'close': close,
        'times': times
    }

h2o.init(verbose=False)
h2o.no_progress()

models_input_directory = '/content/drive/MyDrive/1. learning/1datasci/financial ML/project 1. backtest overfitting forex/results_forex/ALLMODELS_strat1_h2o'

def load_model_paths_with_currency(directory: str) -> dict:
    models = {}

    for model_name in os.listdir(directory):
        currency = model_name.split('_')[0]
        model_path = os.path.join(directory, model_name)

        if currency not in models:
            models[currency] = []

        models[currency].append(model_path)

    return models

loaded_models = load_model_paths_with_currency(models_input_directory)
for currency, model_paths in loaded_models.items():
    print(f"\nCurrency: {currency}")
    for model_path in model_paths:
        model_name = os.path.basename(model_path)
        print(f"  Model: {model_name}")

output_directory = '/content/drive/MyDrive/1. learning/1datasci/financial ML/project 1. backtest overfitting forex/results_forex/BESTMODELS_strat1_h2o'

def form_backtest_paths(predictions):
    paths = {}

    for model_id, model_predictions in predictions.items():
        model_paths = {}
        instance_counters = {}

        group_names = set()
        for split_preds in model_predictions.values():
            group_names.update(split_preds.keys())

        instance_counters = {group: 0 for group in group_names}

        for split, split_preds in model_predictions.items():
            for group_name, preds in split_preds.items():
                current_instance = instance_counters[group_name]
                path_key = f"path_{current_instance + 1}"

                if path_key not in model_paths:
                    model_paths[path_key] = {}

                model_paths[path_key][group_name] = {
                    "predictions": preds["predictions"],
                    "date": preds["date"]
                }

                instance_counters[group_name] += 1

        paths[model_id] = model_paths

    return paths

def identify_best_models_h2o(cv_class, data, times, pct_embargo=0.01, n_groups=5, k_test_groups=2, loaded_models=None):
    if loaded_models is None:
        raise ValueError("loaded_models must be provided")

    if isinstance(cv_class, CombPurgeEmbargoKFold):
        splits = cv_class.create_splits(data, times, n_groups=n_groups, k_test_groups=k_test_groups)
        embargoed_splits = apply_embargo(splits, len(data), pct_embargo=pct_embargo)
        purged_splits = apply_purging(embargoed_splits)
    elif isinstance(cv_class, PurgedKFold):
        splits = cv_class.create_splits(data, times, n_groups=n_groups)
        embargoed_splits = apply_embargo(splits, len(data), pct_embargo=pct_embargo)
        purged_splits = apply_purging(embargoed_splits)
    elif isinstance(cv_class, KFold):
        splits = cv_class.create_splits(data, times, n_groups=n_groups)
        purged_splits = splits
    elif isinstance(cv_class, RollingWindow):
        splits = cv_class.create_splits(data, times, n_groups=n_groups)
        purged_splits = splits

    predictions = {}

    # Loop over currencies and model paths within loaded_models
    for currency, model_paths in loaded_models.items():

        for model_path in model_paths:
            # Extract original model name from the model_path
            original_model_name = os.path.basename(model_path)
            model = h2o.load_model(model_path)
            print(f"  loaded model: {original_model_name}")

            model_predictions = {}

            for i, split in enumerate(purged_splits):
                print(f"    split {i + 1} / {len(purged_splits)}")

                train_indices = split["train_indices"]
                test_indices_dict = split["test_indices"]

                # Prepare H2O training frame
                train_data = data.iloc[train_indices]
                train_h2o = h2o.H2OFrame(train_data)
                y = 'target'  # Assuming 'target' is the column you're predicting
                train_h2o[y] = train_h2o[y].asfactor()

                x = [col for col in train_h2o.columns if col != y]

                # Train model (if necessary, or you can skip training if models are already trained)
                model.train(x=x, y=y, training_frame=train_h2o)

                split_preds = {}
                for group_name, test_indices in test_indices_dict.items():
                    test_data = data.iloc[test_indices]
                    test_h2o = h2o.H2OFrame(test_data)
                    test_h2o[y] = test_h2o[y].asfactor()

                    predictions_h2o = model.predict(test_h2o)
                    pred_df = predictions_h2o.as_data_frame(use_multi_thread=True)

                    start_times = times.iloc[test_indices]["start_time"].values
                    split_preds[group_name] = {
                        "predictions": pred_df,
                        "date": start_times,
                    }

                model_predictions[f"split_{i + 1}"] = split_preds

            # Use the original model name for saving predictions
            predictions[original_model_name] = model_predictions

    # Form backtest paths
    all_paths = form_backtest_paths(predictions)

    concatenated_paths = {}
    for model_key, paths in all_paths.items():
        concatenated_paths[model_key] = {}
        for path_key, groups in paths.items():
            concatenated_predictions = np.concatenate(
                [
                    groups[group_name]["predictions"].values
                    for group_name in groups
                ]
            )
            concatenated_date = np.concatenate(
                [groups[group_name]["date"] for group_name in groups]
            )
            concatenated_paths[model_key][path_key] = {
                "predictions": concatenated_predictions,
                "date": concatenated_date,
            }

    sharpe_ratios = {}
    for model_key, paths in concatenated_paths.items():
        sharpe_ratios[model_key] = {}
        for path_key, data in paths.items():
            predictions = data["predictions"][:, 0]
            date = data["date"]
            positions = np.where(predictions == 1, 1, np.where(predictions == -1, -1, 0))
            positions_series = pd.Series(positions, index=pd.to_datetime(date))
            filtered_close = close[close.index.isin(positions_series.index)].squeeze()
            strat_returns = calc_pnl(filtered_close, positions_series[filtered_close.index])
            sharpe_ratio = calc_sharpe(strat_returns.dropna())
            sharpe_ratios[model_key][path_key] = sharpe_ratio

    all_models_cpcv = []
    for model_key, paths in sharpe_ratios.items():
        row = {"model": model_key}
        for path_key, sharpe_value in paths.items():
            row[path_key] = sharpe_value
        average_sharpe = sum(paths.values()) / len(paths) if paths else 0
        row["average_sharpe"] = average_sharpe
        all_models_cpcv.append(row)

    sharpe_df = pd.DataFrame(all_models_cpcv)
    return sharpe_df

all_results = {}
for currency, data_dict in currency_data.items():
    print('==========================================')
    print(f"\ncurrency: {currency}")

    train = data_dict['train']
    TRAIN_TESTER = train.iloc[8300:11000]
    times = data_dict['times']
    close = data_dict['close']

    # Get the models only for the current currency
    models_for_currency = loaded_models.get(currency, [])

    if not models_for_currency:
        print(f"no models found for {currency}")
        continue

    def process_cv_method(cv_class, cv_name):
        result_df = identify_best_models_h2o(
            cv_class=cv_class,
            data=TRAIN_TESTER,
            times=times,
            pct_embargo=0.01,  # or any value you choose
            n_groups=4,  # can also customize for each CV method
            k_test_groups=2,  # only applicable for some CV methods
            loaded_models={currency: models_for_currency}  # Pass models for this currency
        )
        return result_df

    # Run the function for each CV method and store the results
    cv_methods = {
        "KFold": KFold(),
        "CombPurgeEmbargoKFold": CombPurgeEmbargoKFold(),
    }

    for cv_name, cv_method in cv_methods.items():
        print(f'\n----------------------\n cv method: {cv_name}')
        result_df = process_cv_method(cv_method, cv_name)
        display(result_df)

        # Store the results in a dictionary for later analysis
        if currency not in all_results:
            all_results[currency] = {}
        all_results[currency][cv_name] = result_df

all_results

for currency, cv_results in all_results.items():
    avg_sharpe_df = pd.DataFrame()

    for cv_name, result_df in cv_results.items():
        avg_sharpe = result_df[['model', 'average_sharpe']].copy()

        avg_sharpe['cv_method'] = cv_name

        avg_sharpe_df = pd.concat([avg_sharpe_df, avg_sharpe], ignore_index=True)

    output_path = os.path.join(output_directory, f"{currency}_average_sharpe.csv")
    avg_sharpe_df.to_csv(output_path, index=False)
    print(f"{currency} saved to {output_path}")